model_dir: back-translate/

data: 
    train_features_file: Data1/if_back_train.txt
    train_labels_file: Data1/f_back_train.txt
    eval_features_file: Data1/informal-val.txt
    eval_labels_file: Data1/formal-val.txt
    source_vocabulary: Data1/informal-vocab.txt
    target_vocabulary: Data1/formal-vocab.txt
   
params:
    beam_width: 10

#     optimizer: Adam
#     decay_type: NoamDecay
#     decay_params:
#         model_dim: 512
#         warmup_steps: 5000

#     learning_rate: 0.1
 
eval:
    steps: 250 
    external_evaluators: bleu
    early_stopping:
        metric: bleu
        min_imporvement: 0.05
        steps: 8

train:
    save_checkpoints_steps: 500
    keep_checkpoint_max: 10
    average_last_checkpoints: 10
    max_step: 25000
