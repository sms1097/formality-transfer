{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for Supervised Formality Transfer\n",
    "This was an exceptionally dumb attempt to use a GAN. I thought I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import re \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import workflow_manager as wm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "ENCODER_UNITS = 512\n",
    "DECODER_UNITS = 512\n",
    "ATTENTION_UNITS = 256\n",
    "GENERATOR_UNITS = 1024\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '../Data'\n",
    "train, val, test, context = wm.load_and_tokenize(BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('GAN Seq model weights/input_tokenizer.pickle', 'rb') as handle:\n",
    "    temp = pickle.load(handle)\n",
    "\n",
    "with open('GAN Seq model weights/target_tokenizer.pickle', 'rb') as handle:\n",
    "    other_temp = pickle.load(handle)\n",
    "    \n",
    "assert context['input_tokenizer'].word_index == temp.word_index\n",
    "assert context['target_tokenizer'].word_index == other_temp.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_weights = wm.embedding_matrix(context['input_tokenizer'], \n",
    "                                context['input_vocab_size'], \n",
    "                                BASE_PATH)\n",
    "DE_weights = wm.embedding_matrix(context['target_tokenizer'],\n",
    "                                 context['target_vocab_size'],\n",
    "                                 BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = wm.Encoder(context['input_vocab_size'], EMBEDDING_DIM,\n",
    "                     ENCODER_UNITS, E_weights)\n",
    "decoder = wm.Decoder(context['target_vocab_size'], EMBEDDING_DIM,\n",
    "                     ATTENTION_UNITS, DECODER_UNITS)\n",
    "generator = wm.Generator(GENERATOR_UNITS, context['input_vocab_size'], \n",
    "                         EMBEDDING_DIM)\n",
    "discriminator = wm.Discriminator(context['target_vocab_size'], EMBEDDING_DIM, DE_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7408d80320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_weights('GAN Seq model weights/encoder/encoder')\n",
    "decoder.load_weights('GAN Seq model weights/decoder/decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizers and Loss Functions\n",
    "### Discriminator\n",
    "The loss function for the discriminator is calculated on how well it can discern informal and formal outputs\n",
    "from the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_func(reference, generated):\n",
    "    reference_loss = cross_entropy(tf.ones_like(reference) * 0.8, reference)\n",
    "    generated_loss = cross_entropy(tf.zeros_like(generated) * 0.8, generated)\n",
    "    return tf.reduce_mean(reference_loss + generated_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "This loss only applies to how the BA net did in its efforts to trick the discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss_func(generator_results):\n",
    "    return tf.reduce_mean(cross_entropy(tf.ones_like(generator_results) * 0.8, generator_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "This learns a sequence and then goes through usual GAN paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inpt, trgt):\n",
    "    # This resets the hidden state of the LSTM for every epoch\n",
    "    init_state = [tf.zeros((BATCH_SIZE, ENCODER_UNITS)) for _ in range(4)]\n",
    "\n",
    "    ## Get outputs\n",
    "    gen_input = gen_input = tf.round(\n",
    "        tf.random.uniform(\n",
    "            [BATCH_SIZE, inpt.shape[1]], \n",
    "            minval=1,\n",
    "            maxval=len(context['input_tokenizer'].word_index)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as gtape, tf.GradientTape() as dtape:\n",
    "        # get outputs\n",
    "        gen_output = generator(gen_input)\n",
    "        enc_output, _, _ = encoder(inpt, init_state)\n",
    "        \n",
    "        ## test discriminator\n",
    "        reference_results = discriminator(enc_output, True)\n",
    "        generated_results = discriminator(gen_output, True)\n",
    "        \n",
    "        ## add some noise\n",
    "        reference_results = reference_results  #+ tf.random.normal([BATCH_SIZE, 1])\n",
    "        generated_results = generated_results  # + tf.random.normal([BATCH_SIZE, 1])\n",
    "        \n",
    "\n",
    "        # compute losses\n",
    "        gen_loss = generator_loss_func(generated_results)\n",
    "        disc_loss = discriminator_loss_func(reference_results, generated_results)\n",
    "    \n",
    "    # gradients\n",
    "    discriminator_gradients = dtape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_gradients = gtape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "    \n",
    "    # apply gradients\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, \n",
    "                                                discriminator.trainable_variables))\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Generator Loss 243.5871 | Discriminator Loss 14.7101\n",
      "Time taken 0:05:59.076012\n",
      "\n",
      "Epoch 2 | Generator Loss 221.3409 | Discriminator Loss 13.0750\n",
      "Time taken 0:05:40.775138\n",
      "\n",
      "Epoch 3 | Generator Loss 257.6608 | Discriminator Loss 13.5526\n",
      "Time taken 0:05:40.813919\n",
      "\n",
      "Epoch 4 | Generator Loss 311.0698 | Discriminator Loss 12.9440\n",
      "Time taken 0:05:40.781125\n",
      "\n",
      "Epoch 5 | Generator Loss 330.0643 | Discriminator Loss 12.5536\n",
      "Time taken 0:05:40.723267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = datetime.now()\n",
    "\n",
    "    generator_loss = 0\n",
    "    discriminator_loss = 0\n",
    "    \n",
    "\n",
    "    # This resets the hidden state of the LSTM for every epoch\n",
    "    init_state = [tf.zeros((BATCH_SIZE, ENCODER_UNITS)) for _ in range(4)]\n",
    "\n",
    "    for inpt, trgt in train.take(context['steps_per_epoch']):\n",
    "        batch_gen_loss, batch_disc_loss = train_step(inpt, trgt)\n",
    "        generator_loss += batch_gen_loss\n",
    "        discriminator_loss += batch_disc_loss\n",
    "    \n",
    "    epoch_print = 'Epoch {} | Generator Loss {:.4f} | Discriminator Loss {:.4f}'\n",
    "    \n",
    "    print(epoch_print.format(epoch + 1, tf.reduce_mean(generator_loss).numpy() / BATCH_SIZE, \n",
    "                             tf.reduce_mean(discriminator_loss).numpy() / BATCH_SIZE))\n",
    "    \n",
    "    print('Time taken {}\\n'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inpt, trgt, train=True):\n",
    "    loss = 0\n",
    "    target_tokenizer = context['target_tokenizer']\n",
    "    \n",
    "    # initialize seqs tensor\n",
    "    gen_seqs = tf.constant([target_tokenizer.word_index['<start>']] * BATCH_SIZE, dtype=tf.int64)\n",
    "    gen_seqs = tf.expand_dims(gen_seqs, axis=1)\n",
    "    \n",
    "    # This resets the hidden state of the LSTM for every epoch\n",
    "    init_state = [tf.zeros((BATCH_SIZE, ENCODER_UNITS)) for _ in range(4)]\n",
    "    \n",
    "    gen_input = gen_input = tf.round(\n",
    "        tf.random.uniform(\n",
    "            [BATCH_SIZE, inpt.shape[1]], \n",
    "            minval=1,\n",
    "            maxval=len(context['input_tokenizer'].word_index)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## Generate Sequences\n",
    "    _, h_f, h_b = encoder(inpt, init_state)\n",
    "    enc_output = generator(gen_input)\n",
    "\n",
    "    # Get start token for every sequence in batch\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for i in range(1, trgt.shape[1]):\n",
    "        # dec_hidden shape: (batch_size, decoder_units)\n",
    "        # dec_input shape: (batch_size, 1)\n",
    "        predictions, h_f = decoder(dec_input, h_b, h_f, enc_output)\n",
    "        \n",
    "        # Need to hold onto seqs for discriminator\n",
    "        new_preds = tf.argmax(predictions, axis=1)\n",
    "        new_preds = tf.expand_dims(new_preds, axis=1)\n",
    "        gen_seqs = tf.concat([gen_seqs, new_preds], axis=1)\n",
    "        \n",
    "\n",
    "    if not train:\n",
    "        translated = target_tokenizer.sequences_to_texts(gen_seqs.numpy())\n",
    "        return translated\n",
    "        \n",
    "    return gen_seqs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = predict(x, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> everyone like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> everyone like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb',\n",
       " '<start> <OOV> like everyone dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb dumb']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this GAN has suffered from mode collapse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
