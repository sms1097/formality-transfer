{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formality Classifier\n",
    "This is going to be used to classify whether a sentence should be included in the informal or formal corpus. This will work by selecting the probability of the sentence belonging to the corpus, and if the score exceeds a threshold it will be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wZpo0DbmMOT_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import re \n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qzSYxc4yMOT_"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '../../Data'  # on local is path to directory\n",
    "\n",
    "FORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
    "\n",
    "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
    "\n",
    "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x6EaDkEtMOUA"
   },
   "outputs": [],
   "source": [
    "formal = open(FORMAL_PATH_TRAIN).read()\n",
    "informal = open(INFORMAL_PATH_TRAIN).read()\n",
    "\n",
    "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
    "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fenTZWMxMOUA"
   },
   "outputs": [],
   "source": [
    "def process_sequence(seq):\n",
    "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
    "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    return '<start> ' + s + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6_pLPgB0MOUA"
   },
   "outputs": [],
   "source": [
    "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
    "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
    "\n",
    "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
    "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_corpus = f_corpus.copy()\n",
    "input_corpus.extend(if_corpus)\n",
    "\n",
    "input_labels = [True for _ in range(len(f_corpus))]\n",
    "input_labels.extend([False for _ in range(len(if_corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_corpus = f_holdout.copy()\n",
    "holdout_corpus.extend(if_holdout)\n",
    "\n",
    "holdout_labels = [True for _ in range(len(f_holdout))]\n",
    "holdout_labels.extend([False for _ in range(len(if_holdout))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "czog7VXRMOUA"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
    "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
    "    if not tokenizer: \n",
    "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
    "                              oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    seqs = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return padded_seqs, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, tokenizer = tokenize(input_corpus)\n",
    "test_set, _ = tokenize(holdout_corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "MJpBO_F9MOUA"
   },
   "outputs": [],
   "source": [
    "buffer_size = len(train_set)\n",
    "steps_per_epoch = len(train_set) // BATCH_SIZE\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train_set, input_labels)).shuffle(buffer_size)\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices((test_set, holdout_labels)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Enju-_z_p9u7"
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(tokenizer, vocab_size, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDING_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = embedding_matrix(tokenizer, vocab_size, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[E], mask_zero=True), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)), \n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using keras sequential I developed the model through eager execution. It worked out better this way because I could directly control padding mask and loss function, which is crucial for defining the threshold for when to keep newly generated formal and informal sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormalityClassifier(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FormalityClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[E], mask_zero=True), \n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)), \n",
    "        self.lstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)), \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.ff1 = tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        self.ff2 = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.ff1(x)\n",
    "        x = self.ff2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0P04Qa_MOUA"
   },
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unPfCctsMOUA"
   },
   "source": [
    "Here we define the optimizer and the loss function. In our loss function we mask the zeros since that's the padding.\n",
    "\n",
    "Also of note is in the loss function. The reduction argument at default does some really wonky things which threw off all results. Had to change the reduciton to none, which at default is auto. Not exactly sure what it does in this context but it tries to sum over batches. I didn't work with it because I wanted to control all loss calculation manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kis6sBiBMOUA"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "vnIepemQMOUA"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, preds):\n",
    "    \"\"\"Calculate and return loss\"\"\"\n",
    "\n",
    "    # caclulate loss\n",
    "    loss = static_loss(real, preds)\n",
    "    \n",
    "    # create padding mask \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    \n",
    "    # apply mask\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
