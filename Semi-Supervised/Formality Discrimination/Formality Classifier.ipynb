{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Formality Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMinZFhZPoy"
      },
      "source": [
        "# Formality Classifier\n",
        "This is going to be used to classify whether a sentence should be included in the informal or formal corpus. This will work by selecting the probability of the sentence belonging to the corpus, and if the score exceeds a threshold it will be included. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpo0DbmMOT_"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaBecDtGZPoz"
      },
      "source": [
        "### Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2WlCl45ZPoz"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJWD7o4ZPoz"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDs7s6MIZgs7",
        "outputId": "212744f6-1d91-412a-fec2-0ec264e83691"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "# BASE_PATH = '../../Data'  # on local is path to directory\n",
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
        "\n",
        "# FORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
        "# INFORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Informal_FR_train.txt'.format(BASE_PATH)\n",
        "\n",
        "# FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Formal_FR_ValTest.txt'.format(BASE_PATH)\n",
        "# INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH = BASE_PATH + '/GYAFC_Corpus/Family_Relationships/train/formal'\n",
        "INFORMAL_PATH = BASE_PATH + '/GYAFC_Corpus/Family_Relationships/train/informal'\n",
        "\n",
        "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94bJKxVQ9I9E"
      },
      "source": [
        "formal = open(FORMAL_PATH).read()\n",
        "informal = open(INFORMAL_PATH).read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu1glafMZPoz"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvsjRDf1MzbT"
      },
      "source": [
        "tweeter = TweetTokenizer()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fenTZWMxMOUA"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    # s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    # s = re.sub('\\s{2,}', ' ', s)\n",
        "\n",
        "    t = tweeter.tokenize(seq)\n",
        "    \n",
        "    return '<start> ' + ' '.join(t) + ' <end>'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pLPgB0MOUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c575e332-8a97-48d6-aa7f-83d2adfafa74"
      },
      "source": [
        "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "print(\"There are {} sequences in total\".format(len(f_corpus)))\n",
        "\n",
        "f_val = f_corpus[:2000]\n",
        "if_val = if_corpus[:2000]\n",
        "\n",
        "if_holdout = if_corpus[2000:4000]\n",
        "f_holdout = f_corpus[2000:4000]\n",
        "\n",
        "f_corpus = f_corpus[4000:]\n",
        "if_corpus = if_corpus[4000:]\n",
        "\n",
        "print('Training on {} sequences'.format(len(f_corpus)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 51968 sequences in total\n",
            "Training on 47968 sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HDoSZq0gELy"
      },
      "source": [
        "def split_corpora(formal, informal):\n",
        "    corpus = formal.copy()\n",
        "    corpus.extend(informal)\n",
        "\n",
        "    corpus_labels = [True for _ in range(len(formal))]\n",
        "    corpus_labels.extend([False for _ in range(len(informal))])\n",
        "\n",
        "    return corpus, corpus_labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weeffZBfgdZv"
      },
      "source": [
        "input_corpus, input_labels = split_corpora(f_corpus, if_corpus)\n",
        "holdout_corpus, holdout_labels = split_corpora(f_holdout, if_holdout)\n",
        "val_corpus, val_labels = split_corpora(f_val, if_val)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYOFA9fRZPoz"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='', \n",
        "                              oov_token='<OOV>', lower=False)\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPM9wDXhZPoz"
      },
      "source": [
        "train_set, tokenizer = tokenize(input_corpus)\n",
        "val_set, _ = tokenize(val_corpus, tokenizer)\n",
        "test_set, _ = tokenize(holdout_corpus, tokenizer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_VM2_4LZPoz"
      },
      "source": [
        "### Setup TF dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJpBO_F9MOUA"
      },
      "source": [
        "buffer_size = len(train_set)\n",
        "steps_per_epoch = len(train_set) // BATCH_SIZE\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((train_set, input_labels)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((test_set, holdout_labels))\n",
        "test = test.batch(BATCH_SIZE)\n",
        "\n",
        "val = tf.data.Dataset.from_tensor_slices((val_set, val_labels))\n",
        "val = val.batch(BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT6Z0lzkZPoz"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Bf8jXRZPoz"
      },
      "source": [
        "### Load Embedding Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enju-_z_p9u7"
      },
      "source": [
        "def embedding_matrix(tokenizer, vocab_size, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(EMBEDDING_PATH) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "\n",
        "    return embeddings_matrix"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YZxhplsZPoz"
      },
      "source": [
        "E = embedding_matrix(tokenizer, vocab_size, EMBEDDING_DIM)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR_aCG_3ZPoz"
      },
      "source": [
        "## Declare Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JfJYDkuZPoz"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[E], mask_zero=True),\n",
        "    tf.keras.layers.Dropout(0.8),  \n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024)),\n",
        "    tf.keras.layers.Dropout(0.8), \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-44fwfcZPo0"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHaJfIFZPo0",
        "outputId": "1d11b8b6-332a-4f47-b21f-ab5a8e275e47"
      },
      "source": [
        "history = model.fit(train, validation_data=val, epochs=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1499/1499 [==============================] - 196s 131ms/step - loss: 0.2353 - accuracy: 0.9014 - val_loss: 0.2324 - val_accuracy: 0.8985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPoqEn5ZZPo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127c7c6b-adb7-4da3-ecfd-42fc4f2d8038"
      },
      "source": [
        "model.evaluate(test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 2s 30ms/step - loss: 0.2078 - accuracy: 0.9145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20779110491275787, 0.9144999980926514]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7UwkugTCpy-"
      },
      "source": [
        "Going to find the following data set \n",
        "$$ T_{avg} = \\{(s_i, s_i^\\prime)|P_+(s_i^\\prime) - P_+(s_i) > \\sigma \\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-154K_ZrXfN8"
      },
      "source": [
        "## Import Baseline Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn1wS7IDXeEz"
      },
      "source": [
        "BASELINE_PATH = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "ROUND_TRIP_PATH = '{}/Supervised Data/FD Data/informal_rt.txt'.format(BASE_PATH)\n",
        "SAVE_PATH = '{}/Supervised Data/FD Data/discriminated_seqs.txt'.format(BASE_PATH)\n",
        "IF_APPEND_SAVE_PATH = '{}/Supervised Data/FD Data/discriminated_if_seqs.txt'.format(BASE_PATH)\n",
        "\n",
        "with open(BASELINE_PATH) as f:\n",
        "    if_raw = [process_sequence(seq) for seq in f.read().split('\\n')]\n",
        "\n",
        "with open(ROUND_TRIP_PATH) as f:\n",
        "    if_rt = [process_sequence(seq) for seq in f.read().split('\\n')]\n",
        "\n",
        "if_rt = if_rt[:-1]  # blank line at end of file "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkGHZdk4Y52n"
      },
      "source": [
        "assert len(if_rt) == len(if_raw)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xVdDDfZme9"
      },
      "source": [
        "### Tokenize and Make Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJWRTR9ZZkz"
      },
      "source": [
        "baseline_set, _ = tokenize(if_raw, tokenizer)\n",
        "rt_set, _ = tokenize(if_rt, tokenizer)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQW-ZprWqiz"
      },
      "source": [
        "def get_Tavg(raw, rt, sigma=0.4):\n",
        "    \"\"\"\n",
        "    Take in raw informal sequences and round trip translations \n",
        "    \"\"\"\n",
        "    raw_pred, rt_pred = model.predict(raw), model.predict(rt)\n",
        "    diff = rt_pred - raw_pred\n",
        "    tavg = np.where(diff[:, 1] > sigma)\n",
        "    return tavg[0]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ITlg4fdbpT"
      },
      "source": [
        "tavg = get_Tavg(baseline_set, rt_set)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSkE1FS6flVE"
      },
      "source": [
        "Going to save the addional files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCStzmkPe0X-"
      },
      "source": [
        "with open(ROUND_TRIP_PATH) as f:\n",
        "    temp_rt = [seq for seq in f.read().split('\\n')]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGBGovGWfogQ"
      },
      "source": [
        "additional_seqs = np.array(temp_rt)[tavg]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdHP1WpMfrfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c06b8d9-8478-45fd-f9ee-d2c5f87e19e7"
      },
      "source": [
        "print(\"Expanded dataset by {:2f}%\".format(len(additional_seqs)/25000))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded dataset by 0.089720%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_LFLkdTx66"
      },
      "source": [
        "if_raw_append = list(np.array(if_raw)[tavg])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeGus6Wff9AL"
      },
      "source": [
        "with open(SAVE_PATH, 'w') as f:\n",
        "    for seq in additional_seqs:\n",
        "        f.write(seq + '\\n')\n",
        "\n",
        "with open(IF_APPEND_SAVE_PATH, 'w') as f:\n",
        "    for seq in if_raw_append:\n",
        "        f.write(seq + '\\n')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZGELXzthjZJ"
      },
      "source": [
        "model.save_weights(BASE_PATH + 'formality_classifier')"
      ],
      "execution_count": 82,
      "outputs": []
    }
  ]
}