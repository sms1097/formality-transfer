{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Formality Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMinZFhZPoy"
      },
      "source": [
        "# Formality Classifier\n",
        "This is going to be used to classify whether a sentence should be included in the informal or formal corpus. This will work by selecting the probability of the sentence belonging to the corpus, and if the score exceeds a threshold it will be included. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpo0DbmMOT_"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaBecDtGZPoz"
      },
      "source": [
        "### Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2WlCl45ZPoz"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJWD7o4ZPoz"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDs7s6MIZgs7",
        "outputId": "d9fe898f-2990-4c40-c51b-f2e83e4a4620"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "# BASE_PATH = '../../Data'  # on local is path to directory\n",
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
        "\n",
        "FORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Informal_FR_train.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Formal_FR_ValTest.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
        "\n",
        "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EaDkEtMOUA"
      },
      "source": [
        "formal = open(FORMAL_PATH_TRAIN).read()\n",
        "informal = open(INFORMAL_PATH_TRAIN).read()\n",
        "\n",
        "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
        "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu1glafMZPoz"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fenTZWMxMOUA"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s + ' <end>'"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pLPgB0MOUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca4e6d8-e48b-4ead-bb71-f1eb57c3c772"
      },
      "source": [
        "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
        "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]\n",
        "\n",
        "print(\"Length of holdout set raw is\", len(f_holdout))\n",
        "\n",
        "f_val = f_holdout[:968]\n",
        "if_val = if_holdout[:968]\n",
        "\n",
        "if_holdout = if_holdout[968:]\n",
        "f_holdout = f_holdout[968:]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of holdout set raw is 1968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HDoSZq0gELy"
      },
      "source": [
        "def split_corpora(formal, informal):\n",
        "    corpus = formal.copy()\n",
        "    corpus.extend(informal)\n",
        "\n",
        "    corpus_labels = [True for _ in range(len(formal))]\n",
        "    corpus_labels.extend([False for _ in range(len(informal))])\n",
        "\n",
        "    return corpus, corpus_labels"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weeffZBfgdZv"
      },
      "source": [
        "input_corpus, input_labels = split_corpora(f_corpus, if_corpus)\n",
        "holdout_corpus, holdout_labels = split_corpora(f_holdout, if_holdout)\n",
        "val_corpus, val_labels = split_corpora(f_val, if_val)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYOFA9fRZPoz"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
        "                              oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPM9wDXhZPoz"
      },
      "source": [
        "train_set, tokenizer = tokenize(input_corpus)\n",
        "val_set, _ = tokenize(val_corpus, tokenizer)\n",
        "test_set, _ = tokenize(holdout_corpus, tokenizer)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_VM2_4LZPoz"
      },
      "source": [
        "### Setup TF dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJpBO_F9MOUA"
      },
      "source": [
        "buffer_size = len(train_set)\n",
        "steps_per_epoch = len(train_set) // BATCH_SIZE\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((train_set, input_labels)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((test_set, holdout_labels))\n",
        "test = test.batch(BATCH_SIZE)\n",
        "\n",
        "val = tf.data.Dataset.from_tensor_slices((val_set, val_labels))\n",
        "val = val.batch(BATCH_SIZE)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT6Z0lzkZPoz"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Bf8jXRZPoz"
      },
      "source": [
        "### Load Embedding Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enju-_z_p9u7"
      },
      "source": [
        "def embedding_matrix(tokenizer, vocab_size, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(EMBEDDING_PATH) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "\n",
        "    return embeddings_matrix"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YZxhplsZPoz"
      },
      "source": [
        "E = embedding_matrix(tokenizer, vocab_size, EMBEDDING_DIM)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR_aCG_3ZPoz"
      },
      "source": [
        "## Declare Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JfJYDkuZPoz"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[E], mask_zero=True), \n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024)),\n",
        "    tf.keras.layers.Dropout(0.8), \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-44fwfcZPo0"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHaJfIFZPo0",
        "outputId": "d4a045bf-e8f7-43fe-d1b7-6e6a233e85e1"
      },
      "source": [
        "history = model.fit(train, validation_data=val, epochs=1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "781/781 [==============================] - 58s 74ms/step - loss: 0.5645 - accuracy: 0.6899 - val_loss: 0.4628 - val_accuracy: 0.7660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPoqEn5ZZPo0",
        "outputId": "69c3498b-6221-4f44-e7b2-819f504a6ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.evaluate(test)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 1s 16ms/step - loss: 0.4498 - accuracy: 0.7860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.44977807998657227, 0.7860000133514404]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7UwkugTCpy-"
      },
      "source": [
        "Going to find the following data set \n",
        "$$ T_{avg} = \\{(s_i, s_i^\\prime)|P_+(s_i^\\prime) - P_+(s_i) > \\sigma \\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-154K_ZrXfN8"
      },
      "source": [
        "## Import Baseline Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u48ftSvblZr",
        "outputId": "ce160562-c11d-4eb6-ff3b-2103092def0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.predict(test)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03445356, 0.9655465 ],\n",
              "       [0.10335693, 0.8966431 ],\n",
              "       [0.07642791, 0.9235721 ],\n",
              "       ...,\n",
              "       [0.97287774, 0.02712226],\n",
              "       [0.692602  , 0.30739808],\n",
              "       [0.5367321 , 0.4632679 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn1wS7IDXeEz"
      },
      "source": [
        "BASELINE_PATH = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "ROUND_TRIP_PATH = '{}/Supervised Data/FD Data/informal_rt.txt'.format(BASE_PATH)\n",
        "SAVE_PATH = '{}/Supervised Data/FD Data/discriminated_seqs.txt'.format(BASE_PATH)\n",
        "\n",
        "with open(BASELINE_PATH) as f:\n",
        "    if_raw = [process_sequence(seq) for seq in f.read().split('\\n')]\n",
        "\n",
        "with open(ROUND_TRIP_PATH) as f:\n",
        "    if_rt = [process_sequence(seq) for seq in f.read().split('\\n')]\n",
        "\n",
        "if_rt = if_rt[:-1]  # blank line at end of file "
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkGHZdk4Y52n"
      },
      "source": [
        "assert len(if_rt) == len(if_raw)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0xVdDDfZme9"
      },
      "source": [
        "### Tokenize and Make Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJWRTR9ZZkz"
      },
      "source": [
        "baseline_set, _ = tokenize(if_raw, tokenizer)\n",
        "rt_set, _ = tokenize(if_rt, tokenizer)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_5x9XghbkBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dreD2LVObe9C"
      },
      "source": [
        "x = model.predict(baseline_set)\n",
        "y = model.predict(rt_set)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S578pTCAb7AU",
        "outputId": "1b38ed5b-9877-4f22-9c1d-e3f68dd21ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mce4KXg4cGKG",
        "outputId": "4c799548-a1b1-4937-905f-0ffeb6e96545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y - x"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.05154502, -0.05154508],\n",
              "       [-0.13813248,  0.13813251],\n",
              "       [-0.4590629 ,  0.4590629 ],\n",
              "       ...,\n",
              "       [-0.14580572,  0.14580572],\n",
              "       [ 0.10904804, -0.10904801],\n",
              "       [ 0.        ,  0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeQW-ZprWqiz"
      },
      "source": [
        "def get_Tavg(raw, rt, sigma=0.6):\n",
        "    \"\"\"\n",
        "    Take in raw informal sequences and round trip translations \n",
        "    \"\"\"\n",
        "    raw_pred, rt_pred = model.predict(raw), model.predict(rt)\n",
        "    diff = rt_pred - raw_pred\n",
        "    tavg = np.where(diff[:, 1] > sigma)\n",
        "    return tavg[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ITlg4fdbpT"
      },
      "source": [
        "tavg = get_Tavg(baseline_set, rt_set)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSkE1FS6flVE"
      },
      "source": [
        "Going to save the addional files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCStzmkPe0X-"
      },
      "source": [
        "with open(ROUND_TRIP_PATH) as f:\n",
        "    temp_rt = [seq for seq in f.read().split('\\n')]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGBGovGWfogQ"
      },
      "source": [
        "additional_seqs = np.array(temp_rt)[tavg]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdHP1WpMfrfA",
        "outputId": "50fb29ad-931e-496b-e388-bdb836d93b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(additional_seqs)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeGus6Wff9AL"
      },
      "source": [
        "with open(SAVE_PATH, 'w') as f:\n",
        "    for seq in additional_seqs:\n",
        "        f.write(seq + '\\n')"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZGELXzthjZJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}