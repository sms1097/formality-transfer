{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDMinZFhZPoy"
   },
   "source": [
    "# Formality Classifier\n",
    "This is going to be used to classify whether a sentence should be included in the informal or formal corpus. This will work by selecting the probability of the sentence belonging to the corpus, and if the score exceeds a threshold it will be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZpo0DbmMOT_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import re \n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaBecDtGZPoz"
   },
   "source": [
    "### Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B2WlCl45ZPoz"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPJWD7o4ZPoz"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDs7s6MIZgs7",
    "outputId": "80643336-08f9-4410-826a-dae331516fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qzSYxc4yMOT_"
   },
   "outputs": [],
   "source": [
    "# BASE_PATH = '../../Data'  # on local is path to directory\n",
    "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
    "\n",
    "FORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
    "\n",
    "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Formal_FR_train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Family_Relationships/S_Informal_FR_ValTest.txt'.format(BASE_PATH)\n",
    "\n",
    "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x6EaDkEtMOUA"
   },
   "outputs": [],
   "source": [
    "formal = open(FORMAL_PATH_TRAIN).read()\n",
    "informal = open(INFORMAL_PATH_TRAIN).read()\n",
    "\n",
    "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
    "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu1glafMZPoz"
   },
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fenTZWMxMOUA"
   },
   "outputs": [],
   "source": [
    "def process_sequence(seq):\n",
    "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
    "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    return '<start> ' + s + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6_pLPgB0MOUA"
   },
   "outputs": [],
   "source": [
    "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
    "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
    "\n",
    "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
    "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "31_t-0y5ZPoz"
   },
   "outputs": [],
   "source": [
    "input_corpus = f_corpus.copy()\n",
    "input_corpus.extend(if_corpus)\n",
    "\n",
    "input_labels = [True for _ in range(len(f_corpus))]\n",
    "input_labels.extend([False for _ in range(len(if_corpus))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uu-xm1fLZPoz"
   },
   "outputs": [],
   "source": [
    "holdout_corpus = f_holdout.copy()\n",
    "holdout_corpus.extend(if_holdout)\n",
    "\n",
    "holdout_labels = [True for _ in range(len(f_holdout))]\n",
    "holdout_labels.extend([False for _ in range(len(if_holdout))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYOFA9fRZPoz"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "czog7VXRMOUA"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
    "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
    "    if not tokenizer: \n",
    "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
    "                              oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    seqs = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return padded_seqs, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZPM9wDXhZPoz"
   },
   "outputs": [],
   "source": [
    "train_set, tokenizer = tokenize(input_corpus)\n",
    "test_set, _ = tokenize(holdout_corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_VM2_4LZPoz"
   },
   "source": [
    "### Setup TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MJpBO_F9MOUA"
   },
   "outputs": [],
   "source": [
    "buffer_size = len(train_set)\n",
    "steps_per_epoch = len(train_set) // BATCH_SIZE\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train_set, input_labels)).shuffle(buffer_size)\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices((test_set, holdout_labels))\n",
    "test = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HT6Z0lzkZPoz"
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4Bf8jXRZPoz"
   },
   "source": [
    "### Load Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Enju-_z_p9u7"
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(tokenizer, vocab_size, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDING_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5YZxhplsZPoz"
   },
   "outputs": [],
   "source": [
    "E = embedding_matrix(tokenizer, vocab_size, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR_aCG_3ZPoz"
   },
   "source": [
    "## Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4JfJYDkuZPoz"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weights=[E], mask_zero=True), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True)), \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-44fwfcZPo0"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZHaJfIFZPo0",
    "outputId": "fafd42bb-74b4-4c87-c9e8-df6283d8d939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "421/421 [==============================] - 46s 109ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 2/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9271\n",
      "Epoch 3/10\n",
      "421/421 [==============================] - 46s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 5/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 7/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 8/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 9/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n",
      "Epoch 10/10\n",
      "421/421 [==============================] - 45s 108ms/step - loss: 0.0000e+00 - accuracy: 0.9271\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPoqEn5ZZPo0",
    "outputId": "36816742-b066-4a23-efb1-91700219124f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 5s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.9270246028900146]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhBqLhx0ZPo0"
   },
   "source": [
    "this probably won't need to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyrT3HphZPo0"
   },
   "source": [
    "Instead of using keras sequential I developed the model through eager execution. It worked out better this way because I could directly control padding mask and loss function, which is crucial for defining the threshold for when to keep newly generated formal and informal sequences. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Formality Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
