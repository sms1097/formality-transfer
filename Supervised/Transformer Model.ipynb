{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import re \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "D_MODEL = 512\n",
    "NX = 6\n",
    "H = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qzSYxc4yMOT_"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '../Data'\n",
    "\n",
    "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
    "\n",
    "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x6EaDkEtMOUA"
   },
   "outputs": [],
   "source": [
    "formal = open(FORMAL_PATH_TRAIN).read()\n",
    "informal = open(INFORMAL_PATH_TRAIN).read()\n",
    "\n",
    "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
    "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence(seq):\n",
    "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
    "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    return '<start> ' + s + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_target_input(seq):\n",
    "    \"\"\"\n",
    "    This inserts a space in between the last word and a period\n",
    "    This function covers shifting right for being fed to the Transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    return s + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
    "f_corpus_input = [process_seq_target_input(seq) for seq in formal.split('\\n')]\n",
    "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
    "\n",
    "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
    "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "czog7VXRMOUA"
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
    "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
    "    if not tokenizer: \n",
    "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
    "                              oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    seqs = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return padded_seqs, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_tokenizer = tokenize(if_corpus)\n",
    "target_train, target_tokenizer = tokenize(f_corpus)\n",
    "target_input_train, target_input_tokenizer = tokenize(f_corpus_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_train)\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((input_train, target_input_train, target_train)).shuffle(buffer_size)\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_input_batch, example_target_batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to compute Positional Embeddign from 3.5\n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin(pos, 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} = \\cos(pos, 10000^{2i/d_{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing \n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,...,head_h)W^o$$ \n",
    "where $$head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "and attention is \n",
    "$$ \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In seciton 3.2.3 of AAYN encoder-decoder for seq2seq models keys and values are both form the encoder output, so treating key and value as the same input here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.Model):\n",
    "    def __init__(self, model_size, h):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.mha_size = model_size // h\n",
    "        self.h = h\n",
    "\n",
    "        # learn different weights for all \n",
    "        self.wq = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
    "        self.wo = tf.keras.layers.Dense(model_size)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, i, mask=None):\n",
    "        \"\"\"run for each query, value, key in h\"\"\"\n",
    "        # query shape: (batch_size, query_length, model_size)\n",
    "        # value shape: (batch_size, key_length, model_size)\n",
    "        score = tf.matmul(self.wq[i](q), self.wk[i](k), transpose_b=True)\n",
    "\n",
    "        # eq(1) from AAYN\n",
    "        d_k = tf.math.sqrt(tf.cast(self.mha_size, dtype=tf.float32))\n",
    "\n",
    "        # score shape: (batch_size, query_length, value_length)\n",
    "        score /= d_k\n",
    "\n",
    "        # apply mask\n",
    "        if mask:\n",
    "            score += mask * 1e-8 \n",
    "\n",
    "        # attention shape: (batch_size, query_length, value_length)\n",
    "        attention = tf.nn.softmax(score, axis=2)\n",
    "\n",
    "        # context shape: (batch_size, query_length, value_length)\n",
    "        head = tf.matmul(attention, self.wv[i](k))\n",
    "\n",
    "        return head \n",
    "\n",
    "    def call(self, q, k, mask=None):\n",
    "        \"\"\"This computes the multi head attention by calling for each h\"\"\"\n",
    "        # compute one head attention for each head\n",
    "        multi_head = [self.scaled_dot_product_attention(q, k, i, mask) for i in range(self.h)]\n",
    "        \n",
    "        # concat all heads \n",
    "        multi_head = tf.concat(multi_head, axis=2)\n",
    "\n",
    "        # multi_head shape: (batch_size, query_length, model_size)\n",
    "        mutli_head = self.wo(multi_head)\n",
    "\n",
    "        return mutli_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, h):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h \n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, h)\n",
    "        self.mha_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model, activation='relu')\n",
    "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
    "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, E_out, mask=None):\n",
    "        mha_out = self.mha(E_out, E_out, mask)\n",
    "        mha_out = self.mha_norm(E_out + mha_out)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        FFN_out = self.FFN_l2(self.FFN_l1(mha_out))\n",
    "\n",
    "        #  add and norm\n",
    "        FFN_out = self.FFN_norm(FFN_out + mha_out)\n",
    "        \n",
    "        return FFN_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, h):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.encoder_layers = [EncoderLayer(vocab_size, d_model, h) \n",
    "                               for _ in range(num_layers)]\n",
    "\n",
    "    def positional_encoding(self, pos):\n",
    "        return np.array([\n",
    "            np.sin(pos / 10000 ** (i / self.d_model)) \n",
    "            if i % 2 == 0 else np.cos(pos / 10000 ** (i / self.d_model))\n",
    "            for i in range(self.d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self, seq, mask=None):\n",
    "        \n",
    "        # Embedding Layer\n",
    "        # E_out shape: (batch_size x max_length x d_model)\n",
    "        E_out = self.embedding(seq)\n",
    "        \n",
    "        # Positional Embedding \n",
    "        pes = [self.positional_encoding(i) for i in range(seq.shape[1])]\n",
    "        \n",
    "        # E_out shape: \n",
    "        E_out += pes[:seq.shape[1], :]\n",
    "        \n",
    "        # create x variable\n",
    "        x = E_out\n",
    "\n",
    "        # MultiHeadAttention\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, mask)\n",
    "            \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_vocab_size, D_MODEL, NX, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "\n",
    "        self.mha1 = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.mha1_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "        self.mha2 = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.mha2_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "\n",
    "        self.FFN_l1 = [tf.keras.layers.Dense(4 * model_size) for _ in range(num_layers)]\n",
    "        self.FFN_l2 = [tf.keras.layers.Dense(model_size) for _ in range(num_layers)]\n",
    "        self.FFN_norm = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, seq, enc_opt, mask=None):\n",
    "        E_out = self.embedding(seq)\n",
    "        E_out *= pes[:seq.shape[1], :]\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Define mask\n",
    "            pad_mask = tf.linalg.band_part(tf.ones((len(seq), len(seq))), -1, 0)\n",
    "            \n",
    "            # First MHA layer\n",
    "            mha1_out = self.mha1[i](E_out, E_out, pad_mask)\n",
    "            mha1_out = self.mha1_norm[i](mha1_out + E_out)\n",
    "            \n",
    "            # Second MHA layer\n",
    "            mha2_out = self.mha2[i](E_out, enc_opt, pad_mask)\n",
    "            mha2_out = self.mha2_norm[i](mha2_out + mha1_out)\n",
    "            \n",
    "            # FFN\n",
    "            FFN_out = self.FFN_l2[i](self.FFN_l1[i](mha2_out))\n",
    "            FFN_out = self.FFN_norm[i](FFN_out + mha2_out)\n",
    "        \n",
    "            output = self.fc(FFN_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_sequence = example_input_batch[0]\n",
    "example_output_sequence = example_target_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_vocab_size, D_MODEL, NX, H)\n",
    "decoder = Decoder(target_vocab_size, D_MODEL, NX, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = tf.reshape(example_input_sequence, (1,example_input_sequence.shape[0]))\n",
    "ex1 = tf.reshape(example_output_sequence, (1,example_output_sequence.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2068fe0e368c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-70f635b2110d>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, seq, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# E_out shape:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mE_out\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# create x variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "enc_output = encoder(ex)\n",
    "dec_output = decoder(ex1, enc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "def loss_func(real, preds):\n",
    "    \"\"\"Calculate and return loss\"\"\"\n",
    "    # caclulate loss\n",
    "    loss = static_loss(real, preds)\n",
    "\n",
    "    # create mask \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(in_seq, targ_in_seq, targ_out_seq):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mask = 1 - tf.cast(tf.equal(in_seq, 0), dtype=tf.float32)\n",
    "        enc_opt = encoder(in_seq, mask)\n",
    "        dec_opt = decoder(targ_in_seq, enc_opt, mask)\n",
    "        loss = loss_func(targ_out_seq, dec_opt)\n",
    "        \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss / targ_out_seq.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "start = datetime.now()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss=0\n",
    "    for batch, (inpt, targ_inpt, targ_out) in enumerate(train.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inpt, targ_inpt, targ_out)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                        total_loss/steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} seconds\\n'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
