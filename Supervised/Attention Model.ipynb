{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Attention Model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WZnAymJMOT-"
      },
      "source": [
        "# Encoder-Decoder MT Attention Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8d9mBnhodGq"
      },
      "source": [
        "These are all the papers I reference:\n",
        "\n",
        "\n",
        "1.   [Google Paper](https://arxiv.org/pdf/1609.08144.pdf): This was a study from google on how to configure encoder/decoder networks with attention for machine translation. Most of the decisions I made on what layers to use, how many to use, what attention to use, and other hyper parameters I use are determined from this. I mention most of them as they happen.\n",
        "2.   [Luong et al](https://arxiv.org/pdf/1508.04025.pdf): This is the general scafolding I used for writing the network. This paper is a more digestable than the Bahdanau paper for the general strucutre of writing the seq2seq model.\n",
        "3.   [Bahdanau et al](https://arxiv.org/pdf/1409.0473.pdf): Google referecnes \n",
        "Bahdanau attention as the better decision for attention. Bahdanau attention is referenced in the Luong paper too, but is slightly modified. The difference between the two is explored below. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZpo0DbmMOT_"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIh8TGdFMOT_"
      },
      "source": [
        "### Declare Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLWD_vDUMUbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a72d38b-cdd4-4d2a-c6c3-b96fc495f28b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiCo7jwJMOT_"
      },
      "source": [
        "These parameters are mostly stolen from the Google Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgjKj15QMOT_"
      },
      "source": [
        "EMBEDDING_DIM = 256\n",
        "ATTENTION_UNITS = 512\n",
        "ENCODER_UNITS = 1024\n",
        "DECODER_UNITS = 1024\n",
        "BATCH_SIZE = 64\n",
        "MAX_INPUT_LENGTH = 32"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9sTDg2lMOT_"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'  # on local is path to directory\n",
        "\n",
        "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EaDkEtMOUA"
      },
      "source": [
        "formal = open(FORMAL_PATH_TRAIN).read()\n",
        "informal = open(INFORMAL_PATH_TRAIN).read()\n",
        "\n",
        "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
        "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fenTZWMxMOUA"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s + ' <end>'"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pLPgB0MOUA"
      },
      "source": [
        "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
        "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuet6JWVMOUA"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=None):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
        "                              oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8gCiXWAMOUA"
      },
      "source": [
        "input_train, input_tokenizer = tokenize(if_corpus)\n",
        "target_train, target_tokenizer = tokenize(f_corpus)"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROQQKO8uMOUA"
      },
      "source": [
        "input_test, _ = tokenize(if_holdout, input_tokenizer, 32)\n",
        "target_test, _ = tokenize(f_holdout, target_tokenizer)"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJpBO_F9MOUA"
      },
      "source": [
        "buffer_size = len(input_train)\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((input_test, target_test)).batch(1)"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua_l1fZEMOUA"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGaTdQN0nKP"
      },
      "source": [
        "test_in_batch, test_out_batch = next(iter(test))"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pv7RI8NMOUA"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVMrge8kBd3"
      },
      "source": [
        "Using two bidirectional LSTMs because that was reported to get best performance form the google paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiW3IKWKMOUA"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder_units = encoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm_1 = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(\n",
        "                self.encoder_units,\n",
        "                return_sequences=True\n",
        "            )\n",
        "        )\n",
        "        self.lstm_2 = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(\n",
        "                self.encoder_units,\n",
        "                return_sequences=True,\n",
        "                return_state=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"Shove into latent space\"\"\"\n",
        "        # x shape: (batch_size x max_length x embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape: (batch_size x max_length x 2 * encoder_units)\n",
        "        x = self.lstm_1(x)\n",
        "\n",
        "        # output shape: (batch_size x max_length x 2 * encoder_units)\n",
        "        # h_f, h_b shapes: (batch_size x encoder_units)\n",
        "        output, h_f, _, h_b, _ = self.lstm_2(x)\n",
        "\n",
        "        return output, h_f, h_b\n"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjpvfT3XMOUA"
      },
      "source": [
        "encoder_debug = Encoder(input_vocab_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)\n",
        "\n",
        "init_state = tf.zeros((BATCH_SIZE, ENCODER_UNITS))\n",
        "\n",
        "sample_output, sample_forward_hidden, sample_backward_hidden = encoder_debug(example_input_batch)"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc0BAr9pMOUA"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP5YEjIOMOUA"
      },
      "source": [
        "class GlobalAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    This is called GlobalAttention since that's what\n",
        "    Bahdanau attention is called in the Luong paper\n",
        "    and most of this implementation follows that \n",
        "    scaffolding. \n",
        "\n",
        "    The difference between this and global attention\n",
        "    using concat \n",
        "    \"\"\"\n",
        "    def __init__(self, units):\n",
        "        \"\"\"These parameters follow notation from Bahdanau paper\"\"\"\n",
        "        super(GlobalAttention, self).__init__()\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "        self.U = tf.keras.layers.Dense(units)\n",
        "        self.v = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, enc_opt, hidden_f, hidden_b):\n",
        "        # concatenate hidden states as in eq 7 Bahdanau\n",
        "        # hidden shape: (batch_size, 2 * lstm_units)\n",
        "        hidden = tf.concat([hidden_f, hidden_b], axis=-1)\n",
        "\n",
        "        # expand dims to meet shape of latent tensor\n",
        "        # hidden_broad shape: (batch_size, 1, encoder_units * 2)\n",
        "        hidden_broad = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # Alignment model score from A.1.2 of Bahdanau et al \n",
        "        # score shape: (batch_size, max_length, v_units)\n",
        "        score = self.v(tf.nn.tanh(self.W(hidden_broad) + self.U(enc_opt)))\n",
        "\n",
        "        # softmax generalization of eq(7) Luong\n",
        "        # attention_weights shape: (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)  \n",
        "\n",
        "        # This takes weighted average with attention weights\n",
        "        # context shape: (batch_size, 2 * encoder_units)\n",
        "        context = attention_weights * enc_opt\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "\n",
        "        return context, attention_weights"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7q_BYLQMOUA"
      },
      "source": [
        "attention_layer = GlobalAttention(ATTENTION_UNITS)\n",
        "attention_result, attention_weights = attention_layer(sample_output, \n",
        "                                                      sample_forward_hidden, \n",
        "                                                      sample_backward_hidden)"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBwfe07qMOUA"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-RvXiF1MOUA"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, attention_units, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.attention = GlobalAttention(attention_units)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(decoder_units,\n",
        "                                           return_sequences=True,\n",
        "                                           return_state=True)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.opt = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden_f, hidden_b, encoder_output):\n",
        "        context_vector, attention_weights = self.attention(encoder_output, hidden_f, hidden_b)\n",
        "\n",
        "        # x shape: (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape: (batch_size, 1, embedding_dim + 2 * encoder_units)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # output shape: (batch_size, 1, decoder_units)\n",
        "        # this shape is only important for expanding decoder depth\n",
        "        output, h_f, c_f = self.lstm_1(x)\n",
        "\n",
        "        # flatten to feed into opt\n",
        "        # output shape: (batch_size, hidden_size)\n",
        "        output = self.flatten(output)\n",
        "\n",
        "        # get logits\n",
        "        # x shape: (batch_size, vocab)\n",
        "        x = self.opt(output)\n",
        "\n",
        "        return x, h_f, attention_weights"
      ],
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D1LdlK-MOUA"
      },
      "source": [
        "# decoder = Decoder(target_vocab_size, EMBEDDING_DIM, ATTENTION_UNITS, DECODER_UNITS, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(sample_decoder_input, sample_forward_hidden, sample_backward_hidden, sample_output)"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0P04Qa_MOUA"
      },
      "source": [
        "### Optimizer and Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unPfCctsMOUA"
      },
      "source": [
        "Here we define the optimizer and the loss function. In our loss function we mask the zeros since that's the padding.\n",
        "\n",
        "Also of note is in the loss function. The reduction argument at default does some really wonky things which threw off all results. Had to change the reduciton to none, which at default is auto. Not exactly sure what it does in this context but it tries to sum over batches. I didn't work with it because I wanted to control all loss calculation manually. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kis6sBiBMOUA"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnIepemQMOUA"
      },
      "source": [
        "def loss_function(real, preds):\n",
        "    \"\"\"Calculate and return loss\"\"\"\n",
        "\n",
        "    # caclulate loss\n",
        "    loss = static_loss(real, preds)\n",
        "    \n",
        "    # create padding mask \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    \n",
        "    # apply mask\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lzJ5Y0gMOUB"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOLvx7ygrMtv"
      },
      "source": [
        "This is the training loop. I'm using teacher forcing because I don't think I have enough computational power to use beam search. Google reccomends beam search with a beam width of 10, but that isn't an option here. Teacher forcing will provide better results than without using it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV21GU90MOUB"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inpt, trgt):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, dec_hidden_forward, dec_hidden_backward = encoder(inpt)\n",
        "\n",
        "        # This feeds in the start token for the first initial state\n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for i in range(1, trgt.shape[1]):\n",
        "            # dec_hidden shape: (batch_size, decoder_units)\n",
        "            # dec_input shape: (batch_size, 1)\n",
        "            predictions, dec_hidden_forward, _ = decoder(dec_input, \n",
        "                                                         dec_hidden_forward, \n",
        "                                                         dec_hidden_backward, \n",
        "                                                         enc_output)\n",
        "            \n",
        "            loss += loss_function(trgt[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(trgt[:, 1], 1)\n",
        "\n",
        "        # Apply gradients \n",
        "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "        # return batch loss\n",
        "        return loss"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXRvBFyOMOUB"
      },
      "source": [
        "EPOCHS = 10\n",
        "encoder = Encoder(input_vocab_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)\n",
        "decoder = Decoder(target_vocab_size, EMBEDDING_DIM, ATTENTION_UNITS, \n",
        "                  DECODER_UNITS, BATCH_SIZE)\n",
        "\n",
        "init_state = tf.zeros((BATCH_SIZE, 2 * ENCODER_UNITS))\n",
        "for epoch in range(EPOCHS):\n",
        "    start = datetime.now()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for inpt, trgt in train.take(steps_per_epoch):\n",
        "        total_loss += train_step(inpt, trgt)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / BATCH_SIZE))\n",
        "    print('Time taken {}\\n'.format(datetime.now() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEr2AFkJMOUB"
      },
      "source": [
        "def evaluate(data):\n",
        "    loss = 0\n",
        "    results = []\n",
        "    bleu = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(data):\n",
        "        result = ''\n",
        "        next_word = True\n",
        "\n",
        "        # This feeds in the start token for the first initial state\n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "        \n",
        "        # Get inputs\n",
        "        enc_output, dec_hidden_forward, dec_hidden_backward = encoder(x)\n",
        "\n",
        "        i = 1  # iterative count\n",
        "        while next_word: \n",
        "            # dec_hidden shape: (batch_size, decoder_units)\n",
        "            # dec_input shape: (batch_size, 1)\n",
        "            predictions, dec_hidden_forward, _ = decoder(dec_input, \n",
        "                                                         dec_hidden_forward, \n",
        "                                                         dec_hidden_backward, \n",
        "                                                         enc_output)\n",
        "            loss += loss_function(y[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(predictions, 0)\n",
        "            \n",
        "            # max logit for tokenized word\n",
        "            word_idx = tf.argmax(predictions[0]).numpy()\n",
        "            word = target_tokenizer.index_word[word_idx]\n",
        "            result += word + ' '\n",
        "\n",
        "            if word == '<end>':\n",
        "                next_word = False\n",
        "            i += 1\n",
        "        \n",
        "        results.append(result)\n",
        "        bleu += sentence_bleu(f_holdout[i], result)\n",
        "\n",
        "    return results, loss, bleu / len(f_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vft0SHsNEBNk"
      },
      "source": [
        "results, test_loss, bleu = evaluate(test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}