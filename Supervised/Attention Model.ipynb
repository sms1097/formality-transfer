{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder MT Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import re \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Static Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are mostly stolen from the Google Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "ATTENTION_UNITS = 512\n",
    "ENCODER_UNITS = 1024\n",
    "DECODER_UNITS = 1024\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '../Data/'  # on local is path to directory\n",
    "\n",
    "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
    "\n",
    "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
    "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal = open(FORMAL_PATH_TRAIN).read()\n",
    "informal = open(INFORMAL_PATH_TRAIN).read()\n",
    "\n",
    "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
    "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence(seq):\n",
    "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
    "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    \n",
    "    return '<start> ' + s + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
    "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
    "\n",
    "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
    "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    seqs = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_seqs = pad_sequences(seqs, padding='post')\n",
    "    return padded_seqs, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_tokenizer = tokenize(if_corpus)\n",
    "target_train, target_tokenizer = tokenize(f_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test, in_test_tok = tokenize(if_holdout)\n",
    "target_test, trgt_test_tok = tokenize(f_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_train)\n",
    "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\n",
    "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices((input_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                self.encoder_units,\n",
    "                return_sequences=True\n",
    "            )\n",
    "        )\n",
    "        self.lstm_2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                self.encoder_units,\n",
    "                return_sequences=True,\n",
    "                return_state=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def call(self, x, hidden=None):\n",
    "        \"\"\"Shove into latent space\"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm_1(x, initial_state=hidden)\n",
    "        output, h_f, _, h_b, _ = self.lstm_2(x)\n",
    "\n",
    "        return output, h_f, h_b\n",
    "\n",
    "    def get_initial_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_vocab_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)\n",
    "\n",
    "sample_output, sample_forward_hidden, sample_backward_hidden = encoder(example_input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"These parameters follow notation from Bahdanau paper\"\"\"\n",
    "        super(GlobalAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.U = tf.keras.layers.Dense(units)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, enc_opt, hidden_f, hidden_b):\n",
    "        # concatenate hidden states as in Bahdanau\n",
    "        # hidden shape: (batch_size, 2 * lstm_units)\n",
    "        hidden = tf.concat([hidden_f, hidden_b], axis=-1)\n",
    "\n",
    "        # expand dims to meet shape of latent tensor\n",
    "        # hidden_broad shape: (batch_size, 1, encoder_units * 2)\n",
    "        hidden_broad = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # concat from eq (7.5) Luong (this is Bahdanau scoring, using notation from that paper)\n",
    "        # score shape: (batch_size, max_length, v_units)\n",
    "        score = self.v(tf.nn.tanh(self.W(hidden_broad) + self.U(enc_opt)))\n",
    "\n",
    "        # softmax generalization of eq(7) Luong\n",
    "        # attention_weights shape: (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  \n",
    "\n",
    "        # This takes weighted average with attention weights\n",
    "        # context shape: (batch_size, 2 * encoder_units)\n",
    "        context = attention_weights * enc_opt\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = GlobalAttention(ATTENTION_UNITS)\n",
    "attention_result, attention_weights = attention_layer(sample_output, sample_forward_hidden, sample_backward_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_units, decoder_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.attention = GlobalAttention(attention_units)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(decoder_units,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.opt = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, hidden_f, hidden_b, encoder_output):\n",
    "        context_vector, attention_weights = self.attention(encoder_output, hidden_f, hidden_b)\n",
    "\n",
    "        # x shape: (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape: (batch_size, 1, embedding_dim + 2 * encoder_units)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # output shape: (batch_size, 1, decoder_units)\n",
    "        # this shape is only important for expanding decoder depth\n",
    "        output, h_f, c_f = self.lstm_1(x)\n",
    "\n",
    "        # flatten to feed into opt\n",
    "        # output shape: (batch_size, hidden_size)\n",
    "        output = self.flatten(output)\n",
    "\n",
    "        # get logits\n",
    "        # x shape: (batch_size, vocab)\n",
    "        x = self.opt(output)\n",
    "\n",
    "        return x, h_f, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_vocab_size, EMBEDDING_DIM, ATTENTION_UNITS, DECODER_UNITS, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(sample_decoder_input, sample_forward_hidden, sample_backward_hidden, sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the optimizer and the loss function. In our loss function we mask the zeros since that's the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, preds):\n",
    "    \"\"\"Calculate and return loss\"\"\"\n",
    "\n",
    "    # caclulate loss\n",
    "    loss = static_loss(real, preds)\n",
    "    \n",
    "    # create mask \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    \n",
    "    # apply mask\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inpt, trgt, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, dec_hidden_forward, dec_hidden_backward = encoder(inpt)\n",
    "\n",
    "        # This feeds in the start token for the first initial state\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for i in range(1, trgt.shape[1]):\n",
    "            # dec_hidden shape: (batch_size, decoder_units)\n",
    "            # dec_input shape: (batch_size, 1)\n",
    "            predictions, dec_hidden_forward, _ = decoder(dec_input, \n",
    "                                                         dec_hidden_forward, \n",
    "                                                         dec_hidden_backward, \n",
    "                                                         enc_output)\n",
    "            loss += loss_function(trgt[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(trgt[:, 1], 1)\n",
    "\n",
    "        # Apply gradients \n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        # return batch loss\n",
    "        return loss / trgt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "encoder = Encoder(input_vocab_size, EMBEDDING_DIM, ENCODER_UNITS, BATCH_SIZE)\n",
    "decoder = Decoder(target_vocab_size, EMBEDDING_DIM, ATTENTION_UNITS, DECODER_UNITS, BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    start = datetime.now()\n",
    "\n",
    "    enc_hidden = encoder.get_initial_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inpt, trgt)) in enumerate(train.take(steps_per_epoch)):\n",
    "        total_loss += train_step(inpt, trgt, enc_hidden)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Val_Loss'.format(epoch+1,\n",
    "                                                                  batch,\n",
    "                                                                  batch_loss.numpy()))\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                            total_loss/steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} seconds\\n'.format(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenized_seqs):\n",
    "    enc_output, dec_hidden, _ = encoder(inpt)\n",
    "\n",
    "    \n",
    "    for (inpt, trgt) in tokenized_seqs:\n",
    "        # This feeds in the start token for the first initial state\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "\n",
    "        for i in range(1, trgt.shape[1]):\n",
    "            # dec_hidden shape: (batch_size, decoder_units)\n",
    "            # dec_input shape: (batch_size, 1)\n",
    "            predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(trgt[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(predictions, 1)\n",
    "\n",
    "    # return batch loss\n",
    "    return loss / trgt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
