{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Transformer Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPXV0lvPyO1W"
      },
      "source": [
        "# Attention is All You Need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQy6g9aLyO1X"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbAmNCXOyO1X"
      },
      "source": [
        "### Declare Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIeii5AyO1X"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "D_MODEL = 512\n",
        "MAX_LENGTH = 43\n",
        "NX = 6\n",
        "H = 8\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEQLZTKYyXKK",
        "outputId": "c5037e0e-c801-4b65-ed57-65cb342cda34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j03cA_VCyO1X"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "# BASE_PATH = '../../Data'\n",
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
        "\n",
        "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "\n",
        "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EaDkEtMOUA"
      },
      "source": [
        "formal = open(FORMAL_PATH_TRAIN).read()\n",
        "informal = open(INFORMAL_PATH_TRAIN).read()\n",
        "\n",
        "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
        "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en6uyO_fyO1X"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s + ' <end>'"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jvMi3AiyO1X"
      },
      "source": [
        "def process_seq_target_output(seq):\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return s + ' <end>'"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvS8HQhFJYPE"
      },
      "source": [
        "def process_seq_target_input(seq):\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJcerOWqyO1X"
      },
      "source": [
        "f_corpus = [process_seq_target_output(seq) for seq in formal.split('\\n')]\n",
        "f_corpus_input = [process_seq_target_input(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "f_holdout = [process_seq_target_output(seq) for seq in formal_holdout.split('\\n')]\n",
        "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiGetF_LyO1X"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9yg-riyO1X"
      },
      "source": [
        "This is a little hacky. I force the max length since I already know what it is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=43):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
        "                              oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "        tokenizer.fit_on_texts(['<start>', '<end>'])\n",
        "\n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_q6mke9yO1X"
      },
      "source": [
        "input_train, input_tokenizer = tokenize(if_corpus)\n",
        "target_train, target_tokenizer = tokenize(f_corpus)\n",
        "target_input_train, _ = tokenize(f_corpus_input, target_tokenizer)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6tD-L-syO1X"
      },
      "source": [
        "input_test, _ = tokenize(if_holdout, input_tokenizer)\n",
        "target_test, _ = tokenize(f_holdout, target_tokenizer)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BOqGFc5yO1X"
      },
      "source": [
        "buffer_size = len(input_train)\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((input_train, target_input_train, target_train)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((input_test, target_test)).batch(BATCH_SIZE)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZz0q0yyO1X"
      },
      "source": [
        "example_input_batch, example_target_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heP8u7wzomM5"
      },
      "source": [
        "def embedding_matrix(tokenizer, vocab_size, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(EMBEDDING_PATH) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6b65uZeoyMa"
      },
      "source": [
        "E_enc = embedding_matrix(input_tokenizer, input_vocab_size, EMBEDDING_DIM)\n",
        "E_dec = embedding_matrix(target_tokenizer, target_vocab_size, EMBEDDING_DIM)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1zd93gmqAPf",
        "outputId": "e967b5b2-af48-4a3d-8c42-33b07092feca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "E_enc.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-7adefc7b4359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mE_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCHEMAomyO1X"
      },
      "source": [
        "## Positional Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w87Zj4-ByO1X"
      },
      "source": [
        "Need to compute Positional Embeddign from 3.5\n",
        "\n",
        "$$ PE_{(pos, 2i)} = \\sin(pos, 10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} = \\cos(pos, 10000^{2i/d_{model}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3glgtP-yO1X"
      },
      "source": [
        "def positional_embedding(p, d_model):\n",
        "    p_emb = np.zeros((1, d_model))\n",
        "    for i in range(d_model):\n",
        "        if i % 2 == 0:\n",
        "            p_emb[:, i] = np.sin(p / 10000 ** (i / d_model))\n",
        "        else:\n",
        "            p_emb[:, i] = np.cos(p / 10000 ** (i / d_model))\n",
        "    return p_emb\n",
        "\n",
        "\n",
        "pes = [positional_embedding(i, D_MODEL) for i in range(43)]\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN8wVrWeyO1X"
      },
      "source": [
        "## Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qiE0akyO1X"
      },
      "source": [
        "Computing \n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,...,head_h)W^o$$ \n",
        "where $$head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "and attention is \n",
        "$$ \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anMecoIDyO1X"
      },
      "source": [
        "In seciton 3.2.3 of AAYN encoder-decoder for seq2seq models keys and values are both form the encoder output, so treating key and value as the same input here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES7jt4PPyO1X"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.mha_size = d_model // h\n",
        "        self.h = h\n",
        "\n",
        "        # learn different weights for all \n",
        "        self.wq = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wk = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wv = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, i, mask=None):\n",
        "        \"\"\"run for each query, value, key in h\"\"\"\n",
        "        # query shape: (batch_size, query_length, d_model)\n",
        "        # value shape: (batch_size, key_length, d_model)\n",
        "        score = tf.matmul(self.wq[i](q), self.wk[i](k), transpose_b=True)\n",
        "\n",
        "        # eq(1) from AAYN\n",
        "        d_k = tf.math.sqrt(tf.cast(self.mha_size, dtype=tf.float32))\n",
        "\n",
        "        # score shape: (batch_size, query_length, value_length)\n",
        "        score /= d_k\n",
        "\n",
        "        # mask will \n",
        "        if mask is not None:\n",
        "            score += mask * 1e-8\n",
        "\n",
        "        # attention shape: (batch_size, query_length, value_length)\n",
        "        attention = tf.nn.softmax(score, axis=2)\n",
        "\n",
        "        # context shape: (batch_size, query_length, value_length)\n",
        "        head = tf.matmul(attention, self.wv[i](k))\n",
        "\n",
        "        return head \n",
        "\n",
        "    def call(self, q, k, mask=None):\n",
        "        \"\"\"This computes the multi head attention by calling for each h\"\"\"\n",
        "        # compute one head attention for each head\n",
        "        multi_head = [self.scaled_dot_product_attention(q, k, i, mask) for i in range(self.h)]\n",
        "        \n",
        "        # concat all heads \n",
        "        multi_head = tf.concat(multi_head, axis=2)\n",
        "\n",
        "        # multi_head shape: (batch_size, query_length, model_size)\n",
        "        mutli_head = self.wo(multi_head)\n",
        "\n",
        "        return mutli_head"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SlZR9BnyO1X"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FVgo1v8yO1Y"
      },
      "source": [
        "class EncoderLayer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, h):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h \n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, h)\n",
        "        self.mha_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model, activation='relu')\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, E_out, mask=None):\n",
        "        # MultiHead Attention\n",
        "        mha_out = self.mha(E_out, E_out, mask)\n",
        "        mha_out = self.mha_norm(E_out + mha_out)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha_out))\n",
        "\n",
        "        #  skip and norm\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha_out)\n",
        "        \n",
        "        return FFN_out"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iitVK8duyO1Y"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h, E):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, \n",
        "                                                   weights=[E])\n",
        "        self.encoder_layers = [EncoderLayer(vocab_size, d_model, h) \n",
        "                               for _ in range(num_layers)]\n",
        "        \n",
        "    def call(self, seq, mask=None):\n",
        "        # Embedding Layer\n",
        "        # E_out shape: (batch_size x max_length x d_model)\n",
        "        E_out = self.embedding(seq)\n",
        "\n",
        "        # positonal encoding\n",
        "        x = E_out + pes[:seq.shape[1], :]\n",
        "\n",
        "        # MultiHeadAttention\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layers[i](x, mask)\n",
        "            \n",
        "        return x "
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfX7-aX9yO1Y"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zofx-VxPyO1Y"
      },
      "source": [
        "class DecoderLayer(tf.keras.Model):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, h)\n",
        "        self.mha1_norm = tf.keras.layers.LayerNormalization()\n",
        "        \n",
        "        self.mha2 = MultiHeadAttention(d_model, h)\n",
        "        self.mha2_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model)\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, x, enc_opt, mask=None):\n",
        "\n",
        "        # prevent positions from giving attention to subsequence postions \n",
        "        # (looking ahead) as described in AAYN 3.1 Decoder\n",
        "        # look_ahead_mask shape: (target max length x target max length)\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((x.shape[1], \n",
        "                                                           x.shape[1])), -1, 0)\n",
        "\n",
        "        # First MHA layer\n",
        "        # mha1_out shape: (BATCH_SIZE x target max length)\n",
        "        mha1_out = self.mha1(x, x, look_ahead_mask)\n",
        "        mha1_out = self.mha1_norm(mha1_out + x)\n",
        "\n",
        "        # Second MHA layer\n",
        "        # mha2_out shape: \n",
        "        mha2_out = self.mha2(x, enc_opt, mask)\n",
        "        mha2_out = self.mha2_norm(mha2_out + mha1_out)\n",
        "\n",
        "        # FFN\n",
        "        # FFN_out shape: \n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha2_out))\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha2_out)\n",
        "        \n",
        "        return FFN_out "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-0smDQmyO1Y"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h, E):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, \n",
        "                                                   weights=[E])\n",
        "        self.decoder_layers = [DecoderLayer(d_model, h)\n",
        "                              for _ in range(num_layers)]\n",
        "        self.opt = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, seq, enc_opt, mask=None):\n",
        "        E_out = self.embedding(seq)\n",
        "\n",
        "        x = E_out + pes[:seq.shape[1], :]\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](x, enc_opt)\n",
        "        \n",
        "        output = self.opt(x)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNhWo9MRyO1Y"
      },
      "source": [
        "encoder = Encoder(input_vocab_size, D_MODEL, NX, H, E_enc)\n",
        "decoder = Decoder(target_vocab_size, D_MODEL, NX, H, E_dec)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3aSSJdTyO1Y",
        "outputId": "8149cf3b-e03d-4538-8b9e-43ee40536d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "source": [
        "enc_output = encoder(example_input_batch)\n",
        "dec_output = decoder(example_target_input_batch, enc_output)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-38372566bec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_input_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_target_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-a97905eb17b3>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, seq, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Embedding Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# E_out shape: (batch_size x max_length x d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mE_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# positonal encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m           \u001b[0;31m# Using `init_scope` since we want variable assignment in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m           \u001b[0;31m# `set_weights` to be treated like variable initialization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0mref_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m           raise ValueError(\n\u001b[1;32m   1825\u001b[0m               \u001b[0;34m'Layer weight shape %s not compatible with provided weight '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aoY3BYyO1Y"
      },
      "source": [
        "## Define Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx1edYRJyO1Y"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, preds):\n",
        "    \"\"\"Calculate and return loss\"\"\"\n",
        "    # caclulate loss\n",
        "    loss = static_loss(real, preds)\n",
        "    \n",
        "    # create padding mask \n",
        "    mask = tf.math.logical_not(tf.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    \n",
        "    # apply mask\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klN9xtLeyO1Y"
      },
      "source": [
        "@tf.function\n",
        "def train_step(in_seq, targ_in_seq, targ_out_seq):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # assign 1 to everywhere there is padding\n",
        "        mask = 1 - tf.cast(tf.math.equal(in_seq, 0), dtype=in_seq.dtype)\n",
        "    \n",
        "        # predict\n",
        "        enc_opt = encoder(in_seq)\n",
        "        dec_opt = decoder(targ_in_seq, enc_opt)\n",
        "        loss = loss_function(targ_out_seq, dec_opt)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aK8evQzyO1Y"
      },
      "source": [
        "NUM_EPOCHS = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    start = datetime.now()\n",
        "    for inpt, targ_inpt, targ_out in train.take(steps_per_epoch):\n",
        "        total_loss += train_step(inpt, targ_inpt, targ_out)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / BATCH_SIZE))\n",
        "    print('Time taken for 1 epoch {} seconds\\n'.format(datetime.now() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBxajbiEyO1Y"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMaMudONTneB"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6WT1BDRyO1Y"
      },
      "source": [
        "def evaluate(data):\n",
        "    loss = 0\n",
        "    results = []\n",
        "    bleu = 0\n",
        "    \n",
        "    for i, (x, y) in enumerate(data):\n",
        "        result = '<start>'\n",
        "        next_word = True \n",
        "        \n",
        "        enc_output = encoder(x)\n",
        "        \n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0) \n",
        "        j = 1\n",
        "        while next_word:\n",
        "            prediction = decoder(dec_input, enc_output)\n",
        "            loss += loss_function(y[:, j], prediction)\n",
        "            \n",
        "            # update result\n",
        "            word_idx = tf.argmax(prediction, axis=-1)[:, -1].numpy()[0]\n",
        "            word = target_tokenizer.index_word[word_idx]\n",
        "            result += word + ' '\n",
        "            \n",
        "            # update decoder input\n",
        "            dec_input = tf.concat((dec_input, tf.expand_dims([word_idx], 0)), axis=-1)\n",
        "            \n",
        "            if word == '<end>':\n",
        "                next_word = False\n",
        "                \n",
        "            if j >= y.shape[1] - 1:\n",
        "                result += '<end>'\n",
        "                next_word = False\n",
        "            \n",
        "            j += 1\n",
        "\n",
        "        results.append(result) \n",
        "        bleu += sentence_bleu(f_holdout[i], result)\n",
        "        \n",
        "    return results, loss.numpy() / len(f_holdout), bleu / len(f_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGbYJxzeyO1Y"
      },
      "source": [
        "results, loss, bleu = evaluate(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NthcjLYByO1Z"
      },
      "source": [
        "results[100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDXdIZA4nfqj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}