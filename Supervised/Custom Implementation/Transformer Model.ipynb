{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Transformer Model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihnxmUEC1TB7"
      },
      "source": [
        "# Attention is All You Need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb3fPJiY1TB7"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEyrmLmB1TB7"
      },
      "source": [
        "### Declare Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFHEvoZN1TB7"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "D_MODEL = 512\n",
        "NX = 6\n",
        "H = 8"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2blOFpyB1TB8"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT6EIc2j1cOu",
        "outputId": "10359152-faa2-4918-e4eb-fe04d3415dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "# BASE_PATH = '../../Data'\n",
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
        "\n",
        "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EaDkEtMOUA"
      },
      "source": [
        "formal = open(FORMAL_PATH_TRAIN).read()\n",
        "informal = open(INFORMAL_PATH_TRAIN).read()\n",
        "\n",
        "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
        "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouDB_e7D1TB9"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s + ' <end>'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OjBBDus1TB9"
      },
      "source": [
        "def process_seq_target_input(seq):\n",
        "    \"\"\"\n",
        "    This inserts a space in between the last word and a period\n",
        "    This function covers shifting right for being fed to the Transformer\n",
        "    \"\"\"\n",
        "    \n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return s + ' <end>'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8RYLKmX1TB9"
      },
      "source": [
        "f_corpus = [process_sequence(seq) for seq in formal.split('\\n')]\n",
        "f_corpus_input = [process_seq_target_input(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "f_holdout = [process_sequence(seq) for seq in formal_holdout.split('\\n')]\n",
        "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfxeikdM1TB9"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcoTUjhH1TB9"
      },
      "source": [
        "This is a little hacky. I force the max length since I already know what it is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=43):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
        "                              oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwz1ROAx1TB9"
      },
      "source": [
        "input_train, input_tokenizer = tokenize(if_corpus)\n",
        "target_train, target_tokenizer = tokenize(f_corpus)\n",
        "target_input_train, target_input_tokenizer = tokenize(f_corpus_input)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQllW4Nj1TB9"
      },
      "source": [
        "buffer_size = len(input_train)\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((input_train, target_input_train, target_train)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvSeNpX81TB9"
      },
      "source": [
        "example_input_batch, example_target_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xSfYG2N1TB-"
      },
      "source": [
        "## Positional Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na10s3E21TB-"
      },
      "source": [
        "Need to compute Positional Embeddign from 3.5\n",
        "\n",
        "$$ PE_{(pos, 2i)} = \\sin(pos, 10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} = \\cos(pos, 10000^{2i/d_{model}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4hY3Lhy1TB-",
        "outputId": "4e269b31-1a30-4789-c710-18ce4de7f568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def positional_embedding(p, d_model):\n",
        "    p_emb = np.zeros((1, d_model))\n",
        "    for i in range(d_model):\n",
        "        if i % 2 == 0:\n",
        "            p_emb[:, i] = np.sin(p / 10000 ** (i / d_model))\n",
        "        else:\n",
        "            p_emb[:, i] = np.cos(p / 10000 ** (i / d_model))\n",
        "    return p_emb\n",
        "\n",
        "pes = [positional_embedding(i, D_MODEL) for i in range(43)]\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6105af446fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTK8Whon1TB-"
      },
      "source": [
        "## Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF7yLH1M1TB-"
      },
      "source": [
        "Computing \n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,...,head_h)W^o$$ \n",
        "where $$head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "and attention is \n",
        "$$ \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GleMMFN01TB-"
      },
      "source": [
        "In seciton 3.2.3 of AAYN encoder-decoder for seq2seq models keys and values are both form the encoder output, so treating key and value as the same input here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_orUmAT1TB-"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.mha_size = model_size // h\n",
        "        self.h = h\n",
        "\n",
        "        # learn different weights for all \n",
        "        self.wq = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wk = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wv = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, i, mask=None):\n",
        "        \"\"\"run for each query, value, key in h\"\"\"\n",
        "        # query shape: (batch_size, query_length, model_size)\n",
        "        # value shape: (batch_size, key_length, model_size)\n",
        "        score = tf.matmul(self.wq[i](q), self.wk[i](k), transpose_b=True)\n",
        "\n",
        "        # eq(1) from AAYN\n",
        "        d_k = tf.math.sqrt(tf.cast(self.mha_size, dtype=tf.float32))\n",
        "\n",
        "        # score shape: (batch_size, query_length, value_length)\n",
        "        score /= d_k\n",
        "\n",
        "        # apply mask\n",
        "        if mask:\n",
        "            score += mask * 1e-8\n",
        "\n",
        "        # attention shape: (batch_size, query_length, value_length)\n",
        "        attention = tf.nn.softmax(score, axis=2)\n",
        "\n",
        "        # context shape: (batch_size, query_length, value_length)\n",
        "        head = tf.matmul(attention, self.wv[i](k))\n",
        "\n",
        "        return head \n",
        "\n",
        "    def call(self, q, k, mask=None):\n",
        "        \"\"\"This computes the multi head attention by calling for each h\"\"\"\n",
        "        # compute one head attention for each head\n",
        "        multi_head = [self.scaled_dot_product_attention(q, k, i, mask) for i in range(self.h)]\n",
        "        \n",
        "        # concat all heads \n",
        "        multi_head = tf.concat(multi_head, axis=2)\n",
        "\n",
        "        # multi_head shape: (batch_size, query_length, model_size)\n",
        "        mutli_head = self.wo(multi_head)\n",
        "\n",
        "        return mutli_head"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd8xzTLU1TB-"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUcQWXwI1TB-"
      },
      "source": [
        "class EncoderLayer(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, h):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h \n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, h)\n",
        "        self.mha_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model, activation='relu')\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, E_out, mask=None):\n",
        "        # MultiHead Attention\n",
        "        mha_out = self.mha(E_out, E_out, mask)\n",
        "        mha_out = self.mha_norm(E_out + mha_out)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha_out))\n",
        "\n",
        "        #  add and norm\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha_out)\n",
        "        \n",
        "        return FFN_out"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYnKUf6M1TB-"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.encoder_layers = [EncoderLayer(vocab_size, d_model, h) \n",
        "                               for _ in range(num_layers)]\n",
        "        \n",
        "    def call(self, seq, mask=None):\n",
        "        # Embedding Layer\n",
        "        # E_out shape: (batch_size x max_length x d_model)\n",
        "        E_out = self.embedding(seq)\n",
        "        \n",
        "        # create x variable\n",
        "        x = E_out\n",
        "\n",
        "        # MultiHeadAttention\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layers[i](x, mask)\n",
        "            \n",
        "        return x "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qUNC02k1TB-"
      },
      "source": [
        "encoder = Encoder(input_vocab_size, D_MODEL, NX, H)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo3FaCCp1TB-"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CngP5ch-1TB-"
      },
      "source": [
        "class DecoderLayer(tf.keras.Model):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, h)\n",
        "        self.mha1_norm = tf.keras.layers.LayerNormalization()\n",
        "        \n",
        "        self.mha2 = MultiHeadAttention(d_model, h)\n",
        "        self.mha2_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model)\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, x, enc_opt, mask=None):\n",
        "        # First MHA layer\n",
        "        # mha1_out shape: \n",
        "        mha1_out = self.mha1(x, x)\n",
        "        mha1_out = self.mha1_norm(mha1_out + x)\n",
        "\n",
        "        # Second MHA layer\n",
        "        # mha2_out shape: \n",
        "        mha2_out = self.mha2(x, enc_opt)\n",
        "        mha2_out = self.mha2_norm(mha2_out + mha1_out)\n",
        "\n",
        "        # FFN\n",
        "        # FFN_out shape: \n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha2_out))\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha2_out)\n",
        "        \n",
        "        return FFN_out "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V4ztvov1TB-"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(d_model, h)\n",
        "                              for _ in range(num_layers)]\n",
        "        self.opt = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, seq, enc_opt, mask=None):\n",
        "        x = self.embedding(seq)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](x, enc_opt)\n",
        "        \n",
        "        output = self.opt(x)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yrFRBsV1TB-"
      },
      "source": [
        "example_input_sequence = example_input_batch[0]\n",
        "example_output_sequence = example_target_batch[0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqJ-NKXk1TB-"
      },
      "source": [
        "encoder = Encoder(input_vocab_size, D_MODEL, NX, H)\n",
        "decoder = Decoder(target_vocab_size, D_MODEL, NX, H)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZSbxTF1TB-"
      },
      "source": [
        "ex = tf.reshape(example_input_sequence, (1,example_input_sequence.shape[0]))\n",
        "ex1 = tf.reshape(example_output_sequence, (1,example_output_sequence.shape[0]))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exULxKY41TB-"
      },
      "source": [
        "enc_output = encoder(ex)\n",
        "dec_output = decoder(ex1, enc_output)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VURTOXq1TB-"
      },
      "source": [
        "## Define Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju3jMOc-5P-w"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_func(real, preds):\n",
        "    \"\"\"Calculate and return loss\"\"\"\n",
        "    # caclulate loss\n",
        "    loss = static_loss(real, preds)\n",
        "\n",
        "    # create mask \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwArpMKY1TB-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biqceKDR1TB_"
      },
      "source": [
        "@tf.function\n",
        "def train_step(in_seq, targ_in_seq, targ_out_seq):\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_opt = encoder(in_seq)\n",
        "        dec_opt = decoder(targ_in_seq, enc_opt)\n",
        "        loss = loss_func(targ_out_seq, dec_opt)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return loss / targ_out_seq.shape[1]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnbOZTSo1TB_",
        "outputId": "659a1b4a-4027-429c-c387-e5c5c6cfcb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "NUM_EPOCHS = 2\n",
        "start = datetime.now()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss=0\n",
        "    for batch, (inpt, targ_inpt, targ_out) in enumerate(train.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inpt, targ_inpt, targ_out)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                        total_loss/steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} seconds\\n'.format(datetime.now() - start))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 0.1941\n",
            "Time taken for 1 epoch 0:02:19.990129 seconds\n",
            "\n",
            "Epoch 2 Loss 0.1914\n",
            "Time taken for 1 epoch 0:04:03.599219 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnlsLhO42mDz"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebE5rFRT1TB_"
      },
      "source": [
        "def evaluate():\n",
        "    loss = 0\n",
        "    results = []\n",
        "    bleu = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(data):\n",
        "\n",
        "        result = '<start>'\n",
        "        next_word = True\n",
        "\n",
        "        enc_output = encoder(x)\n",
        "\n",
        "        dec_input = tf.constant([[target_input_tokenizer.word_index['<start>']]])\n",
        "        i = 1  # iterative count\n",
        "        while next_word: \n",
        "\n",
        "            \n",
        "            loss += loss_function(y[:, i], predictions)\n",
        "            \n",
        "            # max logit for tokenized word\n",
        "            word_idx = tf.argmax(predictions[0]).numpy()\n",
        "            word = target_tokenizer.index_word[word_idx]\n",
        "            result += word + ' '\n",
        "\n",
        "            dec_input = tf.expand_dims([word_idx], 0)\n",
        "\n",
        "            if word == '<end>':\n",
        "                next_word = False\n",
        "            \n",
        "            if i >= y.shape[1] - 1:\n",
        "                result += '<end>'\n",
        "                next_word = False\n",
        "            \n",
        "            i += 1\n",
        "\n",
        "        results.append(result)\n",
        "        bleu += sentence_bleu(f_holdout[i], result)\n",
        "\n",
        "    return results, loss.numpy() / len(f_holdout), bleu / len(f_holdout)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajjsi9CO1TB_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}