{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPXV0lvPyO1W"
      },
      "source": [
        "# Attention is All You Need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQy6g9aLyO1X"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "import re \n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbAmNCXOyO1X"
      },
      "source": [
        "### Declare Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIeii5AyO1X"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "D_MODEL = 512\n",
        "MAX_LENGTH = 43\n",
        "NX = 6\n",
        "H = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEQLZTKYyXKK",
        "outputId": "568c1ee0-c131-43c0-e376-c9e7d21f45fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j03cA_VCyO1X"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzSYxc4yMOT_"
      },
      "source": [
        "# BASE_PATH = '../../Data'\n",
        "BASE_PATH = '/content/drive/MyDrive/Data/Data'\n",
        "\n",
        "FORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_Train.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_TRAIN = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_Train.txt'.format(BASE_PATH)\n",
        "\n",
        "FORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Formal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "INFORMAL_PATH_HOLDOUT = '{}/Supervised Data/Entertainment_Music/S_Informal_EM_ValTest.txt'.format(BASE_PATH)\n",
        "\n",
        "EMBEDDING_PATH = '{}/glove.6B.200d.txt'.format(BASE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6EaDkEtMOUA"
      },
      "source": [
        "formal = open(FORMAL_PATH_TRAIN).read()\n",
        "informal = open(INFORMAL_PATH_TRAIN).read()\n",
        "\n",
        "formal_holdout = open(FORMAL_PATH_HOLDOUT).read()\n",
        "informal_holdout = open(INFORMAL_PATH_HOLDOUT).read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en6uyO_fyO1X"
      },
      "source": [
        "def process_sequence(seq):\n",
        "    \"\"\"This inserts a space in between the last word and a period\"\"\"\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jvMi3AiyO1X"
      },
      "source": [
        "def process_seq_target_output(seq):\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return s + ' <end>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvS8HQhFJYPE"
      },
      "source": [
        "def process_seq_target_input(seq):\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', seq)\n",
        "    s = re.sub('\\s{2,}', ' ', s)\n",
        "    \n",
        "    return '<start> ' + s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJcerOWqyO1X"
      },
      "source": [
        "f_corpus = [process_seq_target_output(seq) for seq in formal.split('\\n')]\n",
        "f_corpus_input = [process_seq_target_input(seq) for seq in formal.split('\\n')]\n",
        "if_corpus = [process_sequence(seq) for seq in informal.split('\\n')]\n",
        "\n",
        "f_holdout = [process_seq_target_output(seq) for seq in formal_holdout.split('\\n')]\n",
        "if_holdout = [process_sequence(seq) for seq in informal_holdout.split('\\n')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiGetF_LyO1X"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO9yg-riyO1X"
      },
      "source": [
        "This is a little hacky. I force the max length since I already know what it is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czog7VXRMOUA"
      },
      "source": [
        "def tokenize(corpus, tokenizer=None, maxlen=43):\n",
        "    \"\"\" Tokenize data and pad sequences \"\"\"\n",
        "    if not tokenizer: \n",
        "        tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', \n",
        "                              oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(corpus)\n",
        "        tokenizer.fit_on_texts(['<start>', '<end>'])\n",
        "\n",
        "    seqs = tokenizer.texts_to_sequences(corpus)\n",
        "    padded_seqs = pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "    return padded_seqs, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_q6mke9yO1X"
      },
      "source": [
        "input_train, input_tokenizer = tokenize(if_corpus)\n",
        "target_train, target_tokenizer = tokenize(f_corpus)\n",
        "target_input_train, _ = tokenize(f_corpus_input, target_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6tD-L-syO1X"
      },
      "source": [
        "input_test, _ = tokenize(if_holdout, input_tokenizer)\n",
        "target_test, _ = tokenize(f_holdout, target_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BOqGFc5yO1X"
      },
      "source": [
        "buffer_size = len(input_train)\n",
        "steps_per_epoch = len(input_train) // BATCH_SIZE\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((input_train, target_input_train, target_train)).shuffle(buffer_size)\n",
        "train = train.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test = tf.data.Dataset.from_tensor_slices((input_test, target_test)).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZz0q0yyO1X"
      },
      "source": [
        "example_input_batch, example_target_input_batch, example_target_batch = next(iter(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCHEMAomyO1X"
      },
      "source": [
        "## Positional Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w87Zj4-ByO1X"
      },
      "source": [
        "Need to compute Positional Embeddign from 3.5\n",
        "\n",
        "$$ PE_{(pos, 2i)} = \\sin(pos, 10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} = \\cos(pos, 10000^{2i/d_{model}})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3glgtP-yO1X"
      },
      "source": [
        "def positional_embedding(p, d_model):\n",
        "    p_emb = np.zeros((1, d_model))\n",
        "    for i in range(d_model):\n",
        "        if i % 2 == 0:\n",
        "            p_emb[:, i] = np.sin(p / 10000 ** (i / d_model))\n",
        "        else:\n",
        "            p_emb[:, i] = np.cos(p / 10000 ** (i / d_model))\n",
        "    return p_emb\n",
        "\n",
        "\n",
        "pes = [positional_embedding(i, D_MODEL) for i in range(43)]\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN8wVrWeyO1X"
      },
      "source": [
        "## Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qiE0akyO1X"
      },
      "source": [
        "Computing \n",
        "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,...,head_h)W^o$$ \n",
        "where $$head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "and attention is \n",
        "$$ \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anMecoIDyO1X"
      },
      "source": [
        "In seciton 3.2.3 of AAYN encoder-decoder for seq2seq models keys and values are both form the encoder output, so treating key and value as the same input here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES7jt4PPyO1X"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.mha_size = d_model // h\n",
        "        self.h = h\n",
        "\n",
        "        # learn different weights for all \n",
        "        self.wq = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wk = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wv = [tf.keras.layers.Dense(self.mha_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, i, mask=None):\n",
        "        \"\"\"run for each query, value, key in h\"\"\"\n",
        "        # query shape: (batch_size, query_length, d_model)\n",
        "        # value shape: (batch_size, key_length, d_model)\n",
        "        score = tf.matmul(self.wq[i](q), self.wk[i](k), transpose_b=True)\n",
        "\n",
        "        # eq(1) from AAYN\n",
        "        d_k = tf.math.sqrt(tf.cast(self.mha_size, dtype=tf.float32))\n",
        "\n",
        "        # score shape: (batch_size, query_length, value_length)\n",
        "        score /= d_k\n",
        "\n",
        "        # attention shape: (batch_size, query_length, value_length)\n",
        "        attention = tf.nn.softmax(score, axis=2)\n",
        "\n",
        "        # context shape: (batch_size, query_length, value_length)\n",
        "        head = tf.matmul(attention, self.wv[i](k))\n",
        "\n",
        "        return head \n",
        "\n",
        "    def call(self, q, k, mask=None):\n",
        "        \"\"\"This computes the multi head attention by calling for each h\"\"\"\n",
        "        # compute one head attention for each head\n",
        "        multi_head = [self.scaled_dot_product_attention(q, k, i, mask) for i in range(self.h)]\n",
        "        \n",
        "        # concat all heads \n",
        "        multi_head = tf.concat(multi_head, axis=2)\n",
        "\n",
        "        # multi_head shape: (batch_size, query_length, model_size)\n",
        "        mutli_head = self.wo(multi_head)\n",
        "\n",
        "        return mutli_head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SlZR9BnyO1X"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FVgo1v8yO1Y"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model, h):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h \n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, h)\n",
        "        self.mha_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model, activation='relu')\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, E_out, mask=None):\n",
        "        # MultiHead Attention\n",
        "        mha_out = self.mha(E_out, E_out, mask)\n",
        "        mha_out = self.mha_norm(E_out + mha_out)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha_out))\n",
        "\n",
        "        #  skip and norm\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha_out)\n",
        "        \n",
        "        return FFN_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iitVK8duyO1Y"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.encoder_layers = [EncoderLayer(vocab_size, d_model, h) \n",
        "                               for _ in range(num_layers)]\n",
        "        \n",
        "    def call(self, seq, mask=None):\n",
        "        # Embedding Layer\n",
        "        # E_out shape: (batch_size x max_length x d_model)\n",
        "        E_out = self.embedding(seq)\n",
        "\n",
        "        # positonal encoding\n",
        "        x = E_out + pes[:seq.shape[1], :]\n",
        "\n",
        "        # MultiHeadAttention\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layers[i](x, mask)\n",
        "            \n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfX7-aX9yO1Y"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zofx-VxPyO1Y"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, h):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, h)\n",
        "        self.mha1_norm = tf.keras.layers.LayerNormalization()\n",
        "        \n",
        "        self.mha2 = MultiHeadAttention(d_model, h)\n",
        "        self.mha2_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.FFN_l1 = tf.keras.layers.Dense(4 * d_model)\n",
        "        self.FFN_l2 = tf.keras.layers.Dense(d_model)\n",
        "        self.FFN_norm = tf.keras.layers.LayerNormalization()\n",
        "    \n",
        "    def call(self, x, enc_opt, mask=None):\n",
        "        # First MHA layer\n",
        "        # mha1_out shape: (BATCH_SIZE x target max length)\n",
        "        mha1_out = self.mha1(x, x, look_ahead_mask)\n",
        "        mha1_out = self.mha1_norm(mha1_out + x)\n",
        "\n",
        "        # Second MHA layer\n",
        "        mha2_out = self.mha2(x, enc_opt, mask)\n",
        "        mha2_out = self.mha2_norm(mha2_out + mha1_out)\n",
        "\n",
        "        # FFN\n",
        "        FFN_out = self.FFN_l2(self.FFN_l1(mha2_out))\n",
        "        FFN_out = self.FFN_norm(FFN_out + mha2_out)\n",
        "        \n",
        "        return FFN_out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-0smDQmyO1Y"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.decoder_layers = [DecoderLayer(d_model, h)\n",
        "                              for _ in range(num_layers)]\n",
        "        self.opt = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, seq, enc_opt, mask=None):\n",
        "        E_out = self.embedding(seq)\n",
        "\n",
        "        x = E_out + pes[:seq.shape[1], :]\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i](x, enc_opt)\n",
        "        \n",
        "        output = self.opt(x)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNhWo9MRyO1Y"
      },
      "source": [
        "encoder = Encoder(input_vocab_size, D_MODEL, NX, H)\n",
        "decoder = Decoder(target_vocab_size, D_MODEL, NX, H)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3aSSJdTyO1Y"
      },
      "source": [
        "enc_output = encoder(example_input_batch)\n",
        "dec_output = decoder(example_target_input_batch, enc_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aoY3BYyO1Y"
      },
      "source": [
        "## Define Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx1edYRJyO1Y"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "static_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, preds):\n",
        "    \"\"\"Calculate and return loss\"\"\"\n",
        "    # caclulate loss\n",
        "    loss = static_loss(real, preds)\n",
        "    \n",
        "    # create padding mask \n",
        "    mask = tf.math.logical_not(tf.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    \n",
        "    # apply mask\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klN9xtLeyO1Y"
      },
      "source": [
        "@tf.function\n",
        "def train_step(in_seq, targ_in_seq, targ_out_seq):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # predict\n",
        "        enc_opt = encoder(in_seq)\n",
        "        dec_opt = decoder(targ_in_seq, enc_opt)\n",
        "        loss = loss_function(targ_out_seq, dec_opt)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aK8evQzyO1Y",
        "outputId": "620457a4-d759-41a3-9c40-63872ca763f2"
      },
      "source": [
        "NUM_EPOCHS = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_loss = 0\n",
        "    start = datetime.now()\n",
        "    for inpt, targ_inpt, targ_out in train.take(steps_per_epoch):\n",
        "        total_loss += train_step(inpt, targ_inpt, targ_out)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / BATCH_SIZE))\n",
        "    print('Time taken for 1 epoch {} seconds\\n'.format(datetime.now() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 10.6616\n",
            "Time taken for 1 epoch 0:02:48.664173 seconds\n",
            "\n",
            "Epoch 2 Loss 10.4342\n",
            "Time taken for 1 epoch 0:02:13.017185 seconds\n",
            "\n",
            "Epoch 3 Loss 10.4217\n",
            "Time taken for 1 epoch 0:02:13.301650 seconds\n",
            "\n",
            "Epoch 4 Loss 10.4159\n",
            "Time taken for 1 epoch 0:02:13.320810 seconds\n",
            "\n",
            "Epoch 5 Loss 10.4118\n",
            "Time taken for 1 epoch 0:02:13.698403 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBxajbiEyO1Y"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMaMudONTneB"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6WT1BDRyO1Y"
      },
      "source": [
        "def evaluate(data):\n",
        "    loss = 0\n",
        "    results = []\n",
        "    bleu = 0\n",
        "    \n",
        "    for i, (x, y) in enumerate(data):\n",
        "        result = ''\n",
        "        next_word = True \n",
        "        \n",
        "        enc_output = encoder(x)\n",
        "        \n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0) \n",
        "        j = 1\n",
        "        while next_word:\n",
        "            prediction = decoder(dec_input, enc_output)\n",
        "            loss += loss_function(y[:, j], prediction)\n",
        "            \n",
        "            # update result\n",
        "            word_idx = tf.argmax(prediction, axis=-1)[:, -1].numpy()[0]\n",
        "            word = target_tokenizer.index_word[word_idx]\n",
        "            result += word + ' '\n",
        "            \n",
        "            # update decoder input\n",
        "            dec_input = tf.concat((dec_input, tf.expand_dims([word_idx], 0)), axis=-1)\n",
        "            \n",
        "            if word == '<end>':\n",
        "                next_word = False\n",
        "                \n",
        "            if j >= y.shape[1] - 1:\n",
        "                next_word = False\n",
        "            \n",
        "            j += 1\n",
        "\n",
        "        results.append(result) \n",
        "        bleu += sentence_bleu(f_holdout[i], result)\n",
        "        \n",
        "    return results, loss.numpy() / len(f_holdout), bleu / len(f_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QbFEMBGMxg9"
      },
      "source": [
        "with open(BASE_PATH + '/Results/Custom_Transformer_Results.txt', 'w') as f:\n",
        "    for seq in results:\n",
        "        f.write(seq + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}